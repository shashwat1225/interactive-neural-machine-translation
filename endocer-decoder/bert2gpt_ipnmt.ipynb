{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eea27c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "from transformers import EncoderDecoderModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import nltk\n",
    "from nltk.translate import bleu_score\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bca57af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPUs:  6\n",
      "Using device: cuda:4\n"
     ]
    }
   ],
   "source": [
    "# setup device based on availability\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Total GPUs: \", torch.cuda.device_count())\n",
    "device = \"cuda:4\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ddf9ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "train_source_text_path = \"../data/sup_train.en-fr.fr\"\n",
    "train_target_text_path = \"../data/sup_train.en-fr.en\"\n",
    "dev_source_text_path = \"../data/sup_valid.en-fr.fr\"\n",
    "dev_target_text_path = \"../data/sup_valid.en-fr.en\"\n",
    "test_source_text_path = \"../data/test.en-fr.fr\"\n",
    "test_target_text_path = \"../data/test.en-fr.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfcf9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from the files and store them in pandas dataframe\n",
    "def retrieve_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            data.append(line)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d80feb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_tokenizer = DistilBertTokenizer.from_pretrained('/soe/npullabh/244_final_project/gpt2/tryout/bert2bert_enc_tokenizer')\n",
    "dec_tokenizer = GPT2Tokenizer.from_pretrained('/soe/npullabh/244_final_project/gpt2/tryout/bert2bert_dec_tokenizer')\n",
    "dec_tokenizer.pad_token = dec_tokenizer.eos_token\n",
    "\n",
    "encoder = DistilBertModel.from_pretrained('/soe/npullabh/244_final_project/gpt2/tryout/bert2bert_encoder.pt')\n",
    "decoder = GPT2LMHeadModel.from_pretrained('/soe/npullabh/244_final_project/gpt2/tryout/bert2bert_decoder.pt')\n",
    "model = EncoderDecoderModel(encoder=encoder, decoder=decoder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daadd90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the data\n",
    "def encode_data(source_sentences, target_sentences):\n",
    "    tokenized_inputs = enc_tokenizer.batch_encode_plus(source_sentences, padding='longest', return_tensors='pt')\n",
    "    tokenized_outputs = dec_tokenizer.batch_encode_plus(target_sentences, padding='longest', return_tensors='pt')\n",
    "    \n",
    "    input_ids = tokenized_inputs['input_ids']\n",
    "    input_attention_mask = tokenized_inputs['attention_mask']\n",
    "    labels = tokenized_outputs['input_ids']\n",
    "    output_attention_mask = tokenized_outputs['attention_mask']\n",
    "    \n",
    "    return input_ids, input_attention_mask, labels, output_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c3628a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_source_sentences = retrieve_data(test_source_text_path)\n",
    "test_target_sentences = retrieve_data(test_target_text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62d75fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "test_input_ids, test_attn_mask, test_labels, output_attention_masks = encode_data(test_source_sentences[:16], \n",
    "                                                          test_target_sentences[:16])\n",
    "test_dataset = TensorDataset(test_input_ids, test_attn_mask, test_labels, output_attention_masks)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "645ba03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 139]) torch.Size([16, 139]) torch.Size([16, 59]) torch.Size([16, 59])\n"
     ]
    }
   ],
   "source": [
    "# check if the batching works\n",
    "for _, batch in zip(range(5), test_data_loader):\n",
    "    print(batch[0].shape, batch[1].shape, batch[2].shape, batch[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3ca79b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, tokenizer):\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_prefixes = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for index, batch in tqdm(enumerate(loader), desc=\"Evaluating...\", total=len(loader)):\n",
    "            X = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            y = batch[2].to(device)\n",
    "            decoder_attention_mask = batch[3].to(device)\n",
    "            \n",
    "            y_prefix = y[:, :15]\n",
    "            decoder_attention_mask_prefix = decoder_attention_mask[:, :15]\n",
    "            \n",
    "            print(y.shape, decoder_attention_mask.shape)\n",
    "            \n",
    "            outputs = model.generate(inputs=X, max_length=18, decoder_input_ids=y_prefix, decoder_attention_mask=decoder_attention_mask_prefix)\n",
    "            \n",
    "            predicted_sentences = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "            target_sentences = [tokenizer.decode(target, skip_special_tokens=True) for target in y]\n",
    "            prefixes = [tokenizer.decode(pfx, skip_special_tokens=True) for pfx in y_prefix]\n",
    "            all_predictions.extend(predicted_sentences)\n",
    "            all_targets.extend(target_sentences)\n",
    "            all_prefixes.extend(prefixes)\n",
    "            \n",
    "            loss = model(input_ids=X, decoder_input_ids=y, labels=y).loss\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    \n",
    "    return avg_loss, all_predictions, all_targets, all_prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26cdffdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...:   0%|                                      | 0/1 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Evaluating...: 100%|██████████████████████████████| 1/1 [00:00<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 59]) torch.Size([16, 59])\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_predictions, test_targets, all_prefixes = evaluate(model, test_data_loader, dec_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5caccb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix :  the lega nord in italy, the vlaams blok\n",
      "Pred   :  the lega nord in italy, the vlaams blokokokok\n",
      "Label  :  the lega nord in italy, the vlaams blok in the netherlands, the supporters of le pen's national front in france, are all examples of parties or movements formed on the common theme of aversion to immigrants and promotion of simplistic policies to control them.\n",
      "-----------------\n",
      "Prefix :  this does not mean that the answer is to eliminate heterogeneity and create racially hom\n",
      "Pred   :  this does not mean that the answer is to eliminate heterogeneity and create racially hom hom hom hom\n",
      "Label  :  this does not mean that the answer is to eliminate heterogeneity and create racially homogenous communities, but an acknowledgment of the reality of these issues is needed in order to start constructing solid public policies toward race relations.\n",
      "-----------------\n",
      "Prefix :  some favor affirmative action programs that provide preferences for minorities in job allocation, college\n",
      "Pred   :  some favor affirmative action programs that provide preferences for minorities in job allocation, college college college college\n",
      "Label  :  some favor affirmative action programs that provide preferences for minorities in job allocation, college admission, and public contracts.\n",
      "-----------------\n",
      "Prefix :  while individuals like jorg haidar and jean-marie le\n",
      "Pred   :  while individuals like jorg haidar and jean-marie le le le le\n",
      "Label  :  while individuals like jorg haidar and jean-marie le pen may come and ( never to soon ) go, the race question will not disappear from european politics anytime soon.\n",
      "-----------------\n",
      "Prefix :  this is precisely what a large amount of research in economics, sociology, psychology\n",
      "Pred   :  this is precisely what a large amount of research in economics, sociology, psychology psychology psychology psychology\n",
      "Label  :  this is precisely what a large amount of research in economics, sociology, psychology and political science has done for the us.\n",
      "-----------------\n",
      "Prefix :  europe's divided racial house\n",
      "Pred   :  europe's divided racial houseThe first time\n",
      "Label  :  europe's divided racial house\n",
      "-----------------\n",
      "Prefix :  an aging population at home and ever more open borders imply increasing racial fragmentation in\n",
      "Pred   :  an aging population at home and ever more open borders imply increasing racial fragmentation in in in in\n",
      "Label  :  an aging population at home and ever more open borders imply increasing racial fragmentation in european countries.\n",
      "-----------------\n",
      "Prefix :  a common feature of europe's extreme right is its racism and use\n",
      "Pred   :  a common feature of europe's extreme right is its racism and use use use use\n",
      "Label  :  a common feature of europe's extreme right is its racism and use of the immigration issue as a political wedge.\n",
      "-----------------\n",
      "Prefix :  these policies are seen as a way of offering reparation for past injustices\n",
      "Pred   :  these policies are seen as a way of offering reparation for past injusticesicesicesices\n",
      "Label  :  these policies are seen as a way of offering reparation for past injustices and, more importantly, for creating role models and for overcoming residual and perhaps involuntary discrimination.\n",
      "-----------------\n",
      "Prefix :  of course, americans disagree on how to do this.\n",
      "Pred   :  of course, americans disagree on how to do this.TheTheThe\n",
      "Label  :  of course, americans disagree on how to do this.\n",
      "-----------------\n",
      "Prefix :  the first step to address racial politics is to understand the origin and consequences of\n",
      "Pred   :  the first step to address racial politics is to understand the origin and consequences of of of of\n",
      "Label  :  the first step to address racial politics is to understand the origin and consequences of racial animosity, even if it means uncovering unpleasant truths.\n",
      "-----------------\n",
      "Prefix :  it will not, as america's racial history clearly shows.\n",
      "Pred   :  it will not, as america's racial history clearly shows.TheTheThe\n",
      "Label  :  it will not, as america's racial history clearly shows.\n",
      "-----------------\n",
      "Prefix :  others object to affirmative action, and argue that a race-blind policy\n",
      "Pred   :  others object to affirmative action, and argue that a race-blind policy policy policy policy\n",
      "Label  :  others object to affirmative action, and argue that a race-blind policy coupled with free market polices and pro-family values are all that are necessary to create jobs for minorities and help keep black families together.\n",
      "-----------------\n",
      "Prefix :  this research shows that people of different races trust each other much less ; whites\n",
      "Pred   :  this research shows that people of different races trust each other much less ; whites whites whites whites\n",
      "Label  :  this research shows that people of different races trust each other much less ; whites are less willing to support welfare spending because it is perceived to favor minorities ; more racially fragmented communities have less efficient governments, more corruption and patronage, more crime and fewer productive public goods per tax dollar.\n",
      "-----------------\n",
      "Prefix :  race relations in the us have been for decades - and remain - at the\n",
      "Pred   :  race relations in the us have been for decades - and remain - at the the the the\n",
      "Label  :  race relations in the us have been for decades - and remain - at the center of political debate, to the point that racial cleavages are as important as income, if not more, as determinants of political preferences and attitudes.\n",
      "-----------------\n",
      "Prefix :  mainstream parties of the center left and center right have confronted this prospect by\n",
      "Pred   :  mainstream parties of the center left and center right have confronted this prospect by by by by\n",
      "Label  :  mainstream parties of the center left and center right have confronted this prospect by hiding their heads in the ground, hoping against hope that the problem will disappear.\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_predictions)):\n",
    "    print(\"Prefix : \", all_prefixes[i])\n",
    "    print(\"Pred   : \", test_predictions[i])\n",
    "    print(\"Label  : \", test_targets[i])\n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f126dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 2474, 26420, 5762, 2063, 4487, 11365, 4402, 2139, 1048, 1005, 2885, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]} {'input_ids': [[44252, 431, 705, 82, 220]], 'attention_mask': [[1, 1, 1, 1, 1]]}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m target_input_ids \u001b[38;5;241m=\u001b[39m dec_tokenizer(trg_prefix)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m((source_input_ids), (target_input_ids))\n\u001b[0;32m----> 8\u001b[0m generated \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_input_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_input_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[0;32m/data/users/dhananjay/miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/users/dhananjay/miniconda3/lib/python3.9/site-packages/transformers/generation/utils.py:1223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# 3. Define model inputs\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# inputs_tensor has to be defined\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;66;03m# model_input_name is defined if model-specific keyword input is passed\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;66;03m# otherwise model_input_name is None\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;66;03m# all model-specific keyword inputs are removed from `model_kwargs`\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_model_inputs(\n\u001b[1;32m   1221\u001b[0m     inputs, generation_config\u001b[38;5;241m.\u001b[39mbos_token_id, model_kwargs\n\u001b[1;32m   1222\u001b[0m )\n\u001b[0;32m-> 1223\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[43minputs_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;66;03m# 4. Define other model kwargs\u001b[39;00m\n\u001b[1;32m   1226\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39moutput_attentions\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "src = [\"la maison raciale divisée de l' europe\"]\n",
    "trg_prefix = [\"europe 's \"]\n",
    "source_input_ids = enc_tokenizer(src)\n",
    "target_input_ids = dec_tokenizer(trg_prefix)\n",
    "\n",
    "print((source_input_ids), (target_input_ids))\n",
    "\n",
    "generated = model.generate(inputs=source_input_ids.input_ids, max_length=15, decoder_input_ids=target_input_ids.input_ids)\n",
    "print(generated)\n",
    "print(tokenizer.decode(generated[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f25af28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7991c7c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecdb577d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type t5 to instantiate a model of type encoder-decoder. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Config has to be initialized with encoder and decoder config",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EncoderDecoderModel\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mEncoderDecoderModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt5-small\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencoder([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslate English to German: How are you doing today?\u001b[39m\u001b[38;5;124m'\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m      6\u001b[0m decoder_input_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecoder([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\n",
      "File \u001b[0;32m/data/users/dhananjay/miniconda3/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:379\u001b[0m, in \u001b[0;36mEncoderDecoderModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    373\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    374\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFast initialization is currently not supported for EncoderDecoderModel. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalling back to slow initialization...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    376\u001b[0m     )\n\u001b[1;32m    377\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fast_init\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/users/dhananjay/miniconda3/lib/python3.9/site-packages/transformers/modeling_utils.py:2079\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   2078\u001b[0m     config_path \u001b[38;5;241m=\u001b[39m config \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[0;32m-> 2079\u001b[0m     config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2083\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2084\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2085\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2086\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2087\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2088\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2089\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2090\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_from_auto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_auto_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2091\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2092\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2093\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2094\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2095\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m/data/users/dhananjay/miniconda3/lib/python3.9/site-packages/transformers/configuration_utils.py:545\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type:\n\u001b[1;32m    540\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using a model of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to instantiate a model of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    542\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m     )\n\u001b[0;32m--> 545\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/users/dhananjay/miniconda3/lib/python3.9/site-packages/transformers/configuration_utils.py:688\u001b[0m, in \u001b[0;36mPretrainedConfig.from_dict\u001b[0;34m(cls, config_dict, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    686\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 688\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpruned_heads\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    691\u001b[0m     config\u001b[38;5;241m.\u001b[39mpruned_heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((\u001b[38;5;28mint\u001b[39m(key), value) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mpruned_heads\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[0;32m/data/users/dhananjay/miniconda3/lib/python3.9/site-packages/transformers/models/encoder_decoder/configuration_encoder_decoder.py:77\u001b[0m, in \u001b[0;36mEncoderDecoderConfig.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs\n\u001b[1;32m     79\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfig has to be initialized with encoder and decoder config\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m     encoder_config \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m     encoder_model_type \u001b[38;5;241m=\u001b[39m encoder_config\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Config has to be initialized with encoder and decoder config"
     ]
    }
   ],
   "source": [
    "from transformers import EncoderDecoderModel\n",
    "\n",
    "model = EncoderDecoderModel.from_pretrained('t5-small')\n",
    "\n",
    "input_ids = model.encoder(['translate English to German: How are you doing today?'], return_tensors='pt').input_ids\n",
    "decoder_input_ids = model.decoder(['<pad>'], return_tensors='pt').input_ids\n",
    "decoder_attention_mask = torch.ones(decoder_input_ids.shape, dtype=torch.long, device=model.device) # attention mask for the decoder_input_ids\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    decoder_input_ids=decoder_input_ids,\n",
    "    attention_mask=decoder_attention_mask,\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "generated_text = model.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc22707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b508dc3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04e516d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcf1f10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/mt5-small were not used when initializing MT5Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing MT5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MT5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids, decoder_input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#print(outputs)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/data/users/dhananjay/miniconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3471\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3468\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3469\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3471\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3475\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3476\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/users/dhananjay/miniconda3/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:551\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    550\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 551\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n\u001b[1;32m    554\u001b[0m     clean_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization(text)\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'ids': 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, MT5Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
    "model = MT5Model.from_pretrained(\"google/mt5-small\")\n",
    "input_ids = tokenizer(\n",
    "    \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n",
    "# preprocess: Prepend decoder_input_ids with start token which is pad token for MT5Model.\n",
    "# This is not needed for torch's MT5ForConditionalGeneration as it does this internally using labels arg.\n",
    "decoder_input_ids = model._shift_right(decoder_input_ids)\n",
    "# forward pass\n",
    "outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "#print(outputs)\n",
    "print(tokenizer.decode(outputs.last_hidden_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da984928",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
